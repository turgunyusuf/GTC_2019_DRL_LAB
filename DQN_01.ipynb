{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On #6 - Balancing Cart Pole w/DQN\n",
    "\n",
    "## Goal:\n",
    "* Implement Deep Q-Learning for CartPole\n",
    "    * No digitization\n",
    "    * Handle continuous state\n",
    "\n",
    "## Steps:\n",
    "1. Program Deep Q Learning\n",
    "2. Intro to pytorch\n",
    "3. Metrics to solve Cart Pole\n",
    "4. Plot Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "* My solution for the DQN https://github.com/xsankar/DQN_Navigation/blob/master/Navigation-v2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Organization:\n",
    "\n",
    "### The notebook has 3 parts :\n",
    "\n",
    "* __Part 1__ : Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "* __Part 2__ : Explore and Learn - it performs the DQN Reinforcement Learning. It also saves the best model\n",
    "* __Part 3__ : Run saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the required packages\n",
    "\n",
    "* No esoteric requirements\n",
    "* You can run them without docker\n",
    "* pip install -r requirements.txt\n",
    "* Requirements\n",
    " * python 3.6, pytorch, openAI gym, numpy, matplotlib\n",
    " * anaconda is easier but not needed\n",
    " * Miniconda works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch, gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import gym\n",
    "import PIL # for in-line display of certain environments\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Global Constants and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = 512 # int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "# Number of neurons in the layers of the Q Network\n",
    "FC1_UNITS = 16\n",
    "FC2_UNITS = 8\n",
    "FC3_UNITS = 4\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20943951023931956\n",
      "-0.20943951023931956\n"
     ]
    }
   ],
   "source": [
    "# Work area to quickly test utility functions\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "'''\n",
    "start_time = time.time()\n",
    "time.sleep(10)\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "'''\n",
    "print(math.radians(12))\n",
    "print(math.radians(-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym, PIL\n",
    "# env = gym.make('CartPole-v0')\n",
    "# array = env.reset()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00207654,  0.01987368, -0.01300494,  0.01678067])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "# array = env.reset()\n",
    "# ** render doesn't work reliably on a server. Uncomment when running ** locally **\n",
    "# env.render()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This what it will look like\n",
    "### We don't need the render(). We run it on headless mode and inspect the results\n",
    "<img src=\"CartPole_Render.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "# To see what functions and variables are availabe\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [-0.01130514  0.01678494  0.03270756  0.04122328] ] -> 0  : [ [-0.01096944 -0.17879039  0.03353202  0.34404369] ] R= 1.0\n",
      "[ [-0.01096944 -0.17879039  0.03353202  0.34404369] ] -> 1  : [ [-0.01454525  0.01583888  0.0404129   0.06212045] ] R= 1.0\n",
      "[ [-0.01454525  0.01583888  0.0404129   0.06212045] ] -> 1  : [ [-0.01422847  0.2103588   0.04165531 -0.21754304] ] R= 1.0\n",
      "[ [-0.01422847  0.2103588   0.04165531 -0.21754304] ] -> 0  : [ [-0.0100213   0.0146669   0.03730445  0.08798331] ] R= 1.0\n",
      "[ [-0.0100213   0.0146669   0.03730445  0.08798331] ] -> 1  : [ [-0.00972796  0.20923483  0.03906411 -0.19270057] ] R= 1.0\n",
      "[ [-0.00972796  0.20923483  0.03906411 -0.19270057] ] -> 1  : [ [-0.00554326  0.40377681  0.0352101  -0.472809  ] ] R= 1.0\n",
      "[ [-0.00554326  0.40377681  0.0352101  -0.472809  ] ] -> 1  : [ [ 0.00253227  0.59838425  0.02575392 -0.75418959] ] R= 1.0\n",
      "[ [ 0.00253227  0.59838425  0.02575392 -0.75418959] ] -> 1  : [ [ 0.01449996  0.79314185  0.01067013 -1.0386583 ] ] R= 1.0\n",
      "[ [ 0.01449996  0.79314185  0.01067013 -1.0386583 ] ] -> 1  : [ [ 0.03036279  0.9881204  -0.01010304 -1.32797251] ] R= 1.0\n",
      "[ [ 0.03036279  0.9881204  -0.01010304 -1.32797251] ] -> 1  : [ [ 0.0501252   1.18336839 -0.03666249 -1.6237998 ] ] R= 1.0\n",
      "[ [ 0.0501252   1.18336839 -0.03666249 -1.6237998 ] ] -> 1  : [ [ 0.07379257  1.37890215 -0.06913848 -1.9276797 ] ] R= 1.0\n",
      "[ [ 0.07379257  1.37890215 -0.06913848 -1.9276797 ] ] -> 0  : [ [ 0.10137061  1.184586   -0.10769208 -1.65721236] ] R= 1.0\n",
      "[ [ 0.10137061  1.184586   -0.10769208 -1.65721236] ] -> 0  : [ [ 0.12506233  0.99087236 -0.14083632 -1.39992553] ] R= 1.0\n",
      "[ [ 0.12506233  0.99087236 -0.14083632 -1.39992553] ] -> 1  : [ [ 0.14487978  1.18743572 -0.16883483 -1.73312043] ] R= 1.0\n",
      "[ [ 0.14487978  1.18743572 -0.16883483 -1.73312043] ] -> 0  : [ [ 0.16862849  0.99459535 -0.20349724 -1.49737477] ] R= 1.0\n",
      "[ [ 0.16862849  0.99459535 -0.20349724 -1.49737477] ] -> 0  : [ [ 0.1885204   0.80244411 -0.23344474 -1.27451136] ] R= 1.0\n",
      "Episode 1 finished after 16 steps with a Total Reward = 16\n",
      "[ [ 0.03574345  0.00144424  0.00231929 -0.016103  ] ] -> 0  : [ [ 0.03577233 -0.1937109   0.00199723  0.27731078] ] R= 1.0\n",
      "[ [ 0.03577233 -0.1937109   0.00199723  0.27731078] ] -> 0  : [ [ 0.03189812 -0.38886128  0.00754344  0.57062296] ] R= 1.0\n",
      "[ [ 0.03189812 -0.38886128  0.00754344  0.57062296] ] -> 0  : [ [ 0.02412089 -0.58408821  0.0189559   0.86567277] ] R= 1.0\n",
      "[ [ 0.02412089 -0.58408821  0.0189559   0.86567277] ] -> 1  : [ [ 0.01243913 -0.38922931  0.03626936  0.57900964] ] R= 1.0\n",
      "[ [ 0.01243913 -0.38922931  0.03626936  0.57900964] ] -> 0  : [ [ 0.00465454 -0.58484028  0.04784955  0.88289398] ] R= 1.0\n",
      "[ [ 0.00465454 -0.58484028  0.04784955  0.88289398] ] -> 1  : [ [-0.00704226 -0.39039969  0.06550743  0.60562932] ] R= 1.0\n",
      "[ [-0.00704226 -0.39039969  0.06550743  0.60562932] ] -> 1  : [ [-0.01485026 -0.19625198  0.07762002  0.33427778] ] R= 1.0\n",
      "[ [-0.01485026 -0.19625198  0.07762002  0.33427778] ] -> 0  : [ [-0.0187753  -0.3923879   0.08430557  0.65039322] ] R= 1.0\n",
      "[ [-0.0187753  -0.3923879   0.08430557  0.65039322] ] -> 0  : [ [-0.02662306 -0.58857666  0.09731344  0.96838767] ] R= 1.0\n",
      "[ [-0.02662306 -0.58857666  0.09731344  0.96838767] ] -> 1  : [ [-0.03839459 -0.39488643  0.11668119  0.70779192] ] R= 1.0\n",
      "[ [-0.03839459 -0.39488643  0.11668119  0.70779192] ] -> 1  : [ [-0.04629232 -0.20155753  0.13083703  0.45399687] ] R= 1.0\n",
      "[ [-0.04629232 -0.20155753  0.13083703  0.45399687] ] -> 1  : [ [-0.05032347 -0.00850483  0.13991697  0.20524928] ] R= 1.0\n",
      "[ [-0.05032347 -0.00850483  0.13991697  0.20524928] ] -> 1  : [ [-0.05049356  0.18436785  0.14402195 -0.04023098] ] R= 1.0\n",
      "[ [-0.05049356  0.18436785  0.14402195 -0.04023098] ] -> 0  : [ [-0.04680621 -0.0124942   0.14321733  0.29420107] ] R= 1.0\n",
      "[ [-0.04680621 -0.0124942   0.14321733  0.29420107] ] -> 1  : [ [-0.04705609  0.18032642  0.14910135  0.04989342] ] R= 1.0\n",
      "[ [-0.04705609  0.18032642  0.14910135  0.04989342] ] -> 0  : [ [-0.04344956 -0.01658401  0.15009922  0.38565553] ] R= 1.0\n",
      "[ [-0.04344956 -0.01658401  0.15009922  0.38565553] ] -> 1  : [ [-0.04378124  0.17612388  0.15781233  0.14380749] ] R= 1.0\n",
      "[ [-0.04378124  0.17612388  0.15781233  0.14380749] ] -> 0  : [ [-0.04025877 -0.02086477  0.16068848  0.48182312] ] R= 1.0\n",
      "[ [-0.04025877 -0.02086477  0.16068848  0.48182312] ] -> 1  : [ [-0.04067606  0.17166774  0.17032494  0.24378421] ] R= 1.0\n",
      "[ [-0.04067606  0.17166774  0.17032494  0.24378421] ] -> 1  : [ [-0.03724271  0.36399944  0.17520063  0.00929505] ] R= 1.0\n",
      "[ [-0.03724271  0.36399944  0.17520063  0.00929505] ] -> 0  : [ [-0.02996272  0.16685425  0.17538653  0.35173175] ] R= 1.0\n",
      "[ [-0.02996272  0.16685425  0.17538653  0.35173175] ] -> 1  : [ [-0.02662563  0.35910494  0.18242116  0.11907934] ] R= 1.0\n",
      "[ [-0.02662563  0.35910494  0.18242116  0.11907934] ] -> 1  : [ [-0.01944353  0.55120827  0.18480275 -0.11095955] ] R= 1.0\n",
      "[ [-0.01944353  0.55120827  0.18480275 -0.11095955] ] -> 0  : [ [-0.00841937  0.35398538  0.18258356  0.23386076] ] R= 1.0\n",
      "[ [-0.00841937  0.35398538  0.18258356  0.23386076] ] -> 0  : [ [-0.00133966  0.15678811  0.18726078  0.57812172] ] R= 1.0\n",
      "[ [-0.00133966  0.15678811  0.18726078  0.57812172] ] -> 1  : [ [0.0017961  0.34886042 0.19882321 0.34978347] ] R= 1.0\n",
      "[ [0.0017961  0.34886042 0.19882321 0.34978347] ] -> 0  : [ [0.00877331 0.15154842 0.20581888 0.69799047] ] R= 1.0\n",
      "[ [0.00877331 0.15154842 0.20581888 0.69799047] ] -> 1  : [ [0.01180428 0.34331238 0.21977869 0.47650004] ] R= 1.0\n",
      "Episode 2 finished after 28 steps with a Total Reward = 28\n",
      "[ [-0.00310711 -0.04381299 -0.02644267 -0.04896452] ] -> 1  : [ [-0.00398337  0.15167795 -0.02742196 -0.34987166] ] R= 1.0\n",
      "[ [-0.00398337  0.15167795 -0.02742196 -0.34987166] ] -> 1  : [ [-0.00094981  0.34717895 -0.0344194  -0.65107395] ] R= 1.0\n",
      "[ [-0.00094981  0.34717895 -0.0344194  -0.65107395] ] -> 1  : [ [ 0.00599377  0.54276295 -0.04744088 -0.9543935 ] ] R= 1.0\n",
      "[ [ 0.00599377  0.54276295 -0.04744088 -0.9543935 ] ] -> 1  : [ [ 0.01684903  0.73848991 -0.06652875 -1.261596  ] ] R= 1.0\n",
      "[ [ 0.01684903  0.73848991 -0.06652875 -1.261596  ] ] -> 0  : [ [ 0.03161882  0.54427884 -0.09176067 -0.99046887] ] R= 1.0\n",
      "[ [ 0.03161882  0.54427884 -0.09176067 -0.99046887] ] -> 1  : [ [ 0.0425044   0.74050114 -0.11157004 -1.31050385] ] R= 1.0\n",
      "[ [ 0.0425044   0.74050114 -0.11157004 -1.31050385] ] -> 0  : [ [ 0.05731442  0.54695503 -0.13778012 -1.05472331] ] R= 1.0\n",
      "[ [ 0.05731442  0.54695503 -0.13778012 -1.05472331] ] -> 1  : [ [ 0.06825352  0.74360754 -0.15887459 -1.38728598] ] R= 1.0\n",
      "[ [ 0.06825352  0.74360754 -0.15887459 -1.38728598] ] -> 1  : [ [ 0.08312567  0.94031253 -0.18662031 -1.72514038] ] R= 1.0\n",
      "[ [ 0.08312567  0.94031253 -0.18662031 -1.72514038] ] -> 1  : [ [ 0.10193193  1.137015   -0.22112311 -2.06961949] ] R= 1.0\n",
      "Episode 3 finished after 10 steps with a Total Reward = 10\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render() # Uncomment when running ** locally **. But no need, headless operation is fine\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "### Let us implement a decent Deep Q-Learning Algorithm\n",
    "<img src='DQN_Alg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major components of the algorithm are:\n",
    "1. `A function approximator` implemented as a Deep Neural Network whih consists of fully connected layers. The function approximator learns the Q values for all the actions for a state space. The cartpole environment has a state space of 4 and an action space of 2. So out network has an inputsize of 4 and an output size of 2. It is a simple network.\n",
    "2. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the Q Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent Architecture\n",
    "#### The agent has 3 main components viz:\n",
    "1. The DQN Orchestrator which interacts with the environment by taking actions and then unpacking the returned package to rewards, state space et al.\n",
    "2. It also has to do housekeeping like tracking scores, store high performant models and check when the problem is solved\n",
    "3. The 3rd component is the most interesting one, which gives the agent the capability to select right actions that maximize long term rewards\n",
    " * The fundamental unit is the Q value, which is the total expected reward, discounted for the future actions\n",
    " * The Q value is represented as Q(s,a) i.e. q values for all states for a state. In our case the dimensionality of the state is 4 and 2 actions are possible.\n",
    " * One method is to store all the Q-values in a dictionary with the state and action as key.\n",
    " * Another important concept is the policy i.e. the strategy to choose an action at a given state. We follow the Îµ-greedy policy i.e. for a probability of Îµ, we choose a random action and choose the action that maximizes the total reward otherwise.\n",
    " * So, if we have all the Q values in a table, we can search the actions for every state and then choose the action based on the Îµ-greedy policy. But as the state space becomes larger or even continuous, this table becomes unmanageable and resource consuming even impossible.\n",
    " * Function Approximator â€“ Instead of the Q Table, we can train a function approximator which then can give us the q-value for a given state-action pair.\n",
    " * The function approximator in the DQN algorithm is a neural network, which gives a state returns the q-values for all the actions. This approach is better because, otherwise we need to run the inferencing for each of the action at every state\n",
    "  * Now the challenge is how to train the network. There a few points to explore.\n",
    "   * First is data. Usually the Reinforcement Learning problems involve a sequence of actions and so the Q values are correlated. So then learning run samples from a cache of State-Action-Reward tuple called the experience replay. This gives the network uncorrelated data to learn from. The experience replay buffer is a circular cache\n",
    "  * Second is the training algorithm â€“we use the Fully Connected Neural Networks and train them using SGD (Stochastic Gradient Descent). The actual implementation uses the Adam optimizer. The training algorithm is succinctly captured in the following figure from Udacity.\n",
    "  <img src=\"DQN_sgd.png\">\n",
    "  * In the above diagram, we have no way of getting the target yj. Interestingly this value is obtained from a target network that is exactly same as the expected value network, but updated less frequently. In the actual implementation, it is updated using the exponential decaying technique with the decay constant ð›• = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = FC1_UNITS, fc2_units = FC2_UNITS, \n",
    "                 fc3_units = FC3_UNITS):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcx_units : Number of units in each layer\n",
    "            ToDo : It is a little klugy, as the network is built manually layer-by-layer.\n",
    "                Should take in a list fc_units and then dynamically build the network\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,fc1_units)\n",
    "        # self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        # self.fc3 = nn.Linear(fc2_units,fc3_units)\n",
    "        # self.fc4 = nn.Linear(fc3_units,action_size)\n",
    "        self.fc4 = nn.Linear(fc1_units,action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        #\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.1):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # compute and minimize the loss\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1 - dones)\n",
    "        \n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1,actions)\n",
    "        \n",
    "        # Compute Loss\n",
    "        loss = F.mse_loss(Q_expected,Q_targets)\n",
    "        \n",
    "        #Minimize Loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=42)\n",
    "print(agent.qnetwork_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DQN Algorithm\n",
    "\n",
    "Define the DQN Algorithm. Once we have defined the foundations (network, buffer, agent and so forth), the DQN is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 13 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    has_seen_195 = False\n",
    "    max_score = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()         # reset the environment\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action) # send the action to the environment\n",
    "            #\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            max_steps += 1\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "              format(i_episode, np.mean(scores_window),max_steps,eps,max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "                  format(i_episode, np.mean(scores_window),max_steps,eps,max_score))\n",
    "        if (np.mean(scores_window)>=195.0) and (not has_seen_195):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_195 = True\n",
    "            break # early stop gives perfect score during testing !!\n",
    "            # To see how far it can go we can comment the break. But it didn't work out for me.\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DQN\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 100\tAverage Score : 22.92\tMax_steps : 26\teps : 0.951\tMax.Score : 71.000\n",
      "Episode : 200\tAverage Score : 20.21\tMax_steps : 15\teps : 0.905\tMax.Score : 80.000\n",
      "Episode : 300\tAverage Score : 21.73\tMax_steps : 12\teps : 0.861\tMax.Score : 93.000\n",
      "Episode : 400\tAverage Score : 22.34\tMax_steps : 16\teps : 0.819\tMax.Score : 93.000\n",
      "Episode : 500\tAverage Score : 22.72\tMax_steps : 10\teps : 0.779\tMax.Score : 97.000\n",
      "Episode : 600\tAverage Score : 26.09\tMax_steps : 37\teps : 0.741\tMax.Score : 99.000\n",
      "Episode : 700\tAverage Score : 30.28\tMax_steps : 18\teps : 0.705\tMax.Score : 99.000\n",
      "Episode : 800\tAverage Score : 30.64\tMax_steps : 25\teps : 0.670\tMax.Score : 99.000\n",
      "Episode : 900\tAverage Score : 46.54\tMax_steps : 148\teps : 0.638\tMax.Score : 158.000\n",
      "Episode : 1000\tAverage Score : 72.23\tMax_steps : 101\teps : 0.606\tMax.Score : 168.000\n",
      "Episode : 1100\tAverage Score : 75.40\tMax_steps : 40\teps : 0.577\tMax.Score : 200.0000\n",
      "Episode : 1200\tAverage Score : 93.07\tMax_steps : 200\teps : 0.549\tMax.Score : 200.000\n",
      "Episode : 1300\tAverage Score : 109.59\tMax_steps : 179\teps : 0.522\tMax.Score : 200.000\n",
      "Episode : 1400\tAverage Score : 111.97\tMax_steps : 39\teps : 0.496\tMax.Score : 200.0000\n",
      "Episode : 1500\tAverage Score : 136.98\tMax_steps : 169\teps : 0.472\tMax.Score : 200.000\n",
      "Episode : 1600\tAverage Score : 138.50\tMax_steps : 127\teps : 0.449\tMax.Score : 200.000\n",
      "Episode : 1700\tAverage Score : 145.75\tMax_steps : 123\teps : 0.427\tMax.Score : 200.000\n",
      "Episode : 1800\tAverage Score : 158.15\tMax_steps : 198\teps : 0.406\tMax.Score : 200.000\n",
      "Episode : 1900\tAverage Score : 153.22\tMax_steps : 77\teps : 0.387\tMax.Score : 200.0000\n",
      "Episode : 2000\tAverage Score : 152.44\tMax_steps : 156\teps : 0.368\tMax.Score : 200.000\n",
      "Episode : 2100\tAverage Score : 177.95\tMax_steps : 156\teps : 0.350\tMax.Score : 200.000\n",
      "Episode : 2200\tAverage Score : 177.00\tMax_steps : 200\teps : 0.333\tMax.Score : 200.000\n",
      "Episode : 2300\tAverage Score : 180.27\tMax_steps : 200\teps : 0.317\tMax.Score : 200.000\n",
      "Episode : 2400\tAverage Score : 187.87\tMax_steps : 200\teps : 0.301\tMax.Score : 200.000\n",
      "Episode : 2500\tAverage Score : 187.66\tMax_steps : 200\teps : 0.286\tMax.Score : 200.000\n",
      "Episode : 2600\tAverage Score : 182.57\tMax_steps : 148\teps : 0.272\tMax.Score : 200.000\n",
      "Episode : 2700\tAverage Score : 171.99\tMax_steps : 200\teps : 0.259\tMax.Score : 200.000\n",
      "Episode : 2800\tAverage Score : 183.69\tMax_steps : 200\teps : 0.247\tMax.Score : 200.000\n",
      "Episode : 2900\tAverage Score : 192.24\tMax_steps : 200\teps : 0.234\tMax.Score : 200.000\n",
      "Episode : 3000\tAverage Score : 193.30\tMax_steps : 191\teps : 0.223\tMax.Score : 200.000\n",
      "Episode : 3027\tAverage Score : 195.50\tMax_steps : 200\teps : 0.220\tMax.Score : 200.000\n",
      "Environment solved in 2927 episodes!\tAverage Score: 195.50\n",
      "Elapsed : 0:03:21.086424\n",
      "2019-02-08 10:47:42.104087\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Max Score 200.000000 at 1035\n",
      "Percentile [25,50,75] : [ 31. 128. 200.]\n",
      "Variance : 5854.559\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = dqn(n_episodes=5000,eps_end=0.01, eps_decay=0.9995) \n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.qnetwork_local)\n",
    "print('Max Score {:2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Run\n",
    "#### Usually solves in ~4000 steps. \n",
    "#### It can do it in ~2400, but misses and unlearns\n",
    "---\n",
    "<img src=\"DQN_Solution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Test - Run a stored Model or test the current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the state dict, because we have access to the code.\n",
    "\n",
    "- `torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')`\n",
    "- `agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "The best way to save and load model, to be used by 2 distinct and separate entities is to :\n",
    "- `torch.save(model, filepath)`; \n",
    "- Then later, `model = torch.load(filepath)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to Ponder\n",
    "1. There is a form-fit equilibrium for every Deep Learning problem\n",
    "2. I tried with a replay buffer of 100,000 and a network FC16-FC8-FC4-FC2\n",
    "    * This couldn't find it's way around even after 100,000 episodes. It would just meander !\n",
    "3. Reduced the buffer to 512, minibatch size = 64 (as before) and a tiny network FC16-FC2\n",
    "    * Worked 1st time, solved the environment in 2582 episodes\n",
    "    * I had set the number of episodes as 5,000. The network then was able to get a score of only ~123 during the test below.\n",
    "4. The env cuts off after 200 episodes. So we can't get a score higher than 200\n",
    "5. An early stop gives perferct 200 in the test below !! \n",
    "    * We have to assume that the network did learn a trick or two !!\n",
    "    * And no digitization and the network can handle continuous state space (for co ntinuous action soace we need DDPG!)\n",
    "<img src=\"Confidence.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1 Score 200.00\n",
      "Episode  2 Score 200.00\n",
      "Episode  3 Score 200.00\n",
      "Episode  4 Score 200.00\n",
      "Episode  5 Score 200.00\n",
      "Episode  6 Score 200.00\n",
      "Episode  7 Score 200.00\n",
      "Episode  8 Score 200.00\n",
      "Episode  9 Score 200.00\n",
      "Episode 10 Score 200.00\n",
      "Episode 11 Score 200.00\n",
      "Episode 12 Score 200.00\n",
      "Episode 13 Score 200.00\n",
      "Episode 14 Score 200.00\n",
      "Episode 15 Score 200.00\n",
      "Episode 16 Score 200.00\n",
      "Episode 17 Score 200.00\n",
      "Episode 18 Score 200.00\n",
      "Episode 19 Score 200.00\n",
      "Episode 20 Score 200.00\n",
      "Episode 21 Score 200.00\n",
      "Episode 22 Score 200.00\n",
      "Episode 23 Score 200.00\n",
      "Episode 24 Score 200.00\n",
      "Episode 25 Score 200.00\n",
      "Episode 26 Score 200.00\n",
      "Episode 27 Score 200.00\n",
      "Episode 28 Score 200.00\n",
      "Episode 29 Score 200.00\n",
      "Episode 30 Score 200.00\n",
      "Episode 31 Score 200.00\n",
      "Episode 32 Score 200.00\n",
      "Episode 33 Score 200.00\n",
      "Episode 34 Score 200.00\n",
      "Episode 35 Score 200.00\n",
      "Episode 36 Score 200.00\n",
      "Episode 37 Score 200.00\n",
      "Episode 38 Score 200.00\n",
      "Episode 39 Score 200.00\n",
      "Episode 40 Score 200.00\n",
      "Episode 41 Score 200.00\n",
      "Episode 42 Score 200.00\n",
      "Episode 43 Score 200.00\n",
      "Episode 44 Score 200.00\n",
      "Episode 45 Score 200.00\n",
      "Episode 46 Score 200.00\n",
      "Episode 47 Score 200.00\n",
      "Episode 48 Score 200.00\n",
      "Episode 49 Score 200.00\n",
      "Episode 50 Score 200.00\n",
      "Episode 51 Score 200.00\n",
      "Episode 52 Score 200.00\n",
      "Episode 53 Score 200.00\n",
      "Episode 54 Score 200.00\n",
      "Episode 55 Score 200.00\n",
      "Episode 56 Score 200.00\n",
      "Episode 57 Score 200.00\n",
      "Episode 58 Score 200.00\n",
      "Episode 59 Score 200.00\n",
      "Episode 60 Score 200.00\n",
      "Episode 61 Score 200.00\n",
      "Episode 62 Score 200.00\n",
      "Episode 63 Score 200.00\n",
      "Episode 64 Score 200.00\n",
      "Episode 65 Score 200.00\n",
      "Episode 66 Score 200.00\n",
      "Episode 67 Score 200.00\n",
      "Episode 68 Score 200.00\n",
      "Episode 69 Score 200.00\n",
      "Episode 70 Score 200.00\n",
      "Episode 71 Score 200.00\n",
      "Episode 72 Score 200.00\n",
      "Episode 73 Score 200.00\n",
      "Episode 74 Score 200.00\n",
      "Episode 75 Score 200.00\n",
      "Episode 76 Score 200.00\n",
      "Episode 77 Score 200.00\n",
      "Episode 78 Score 200.00\n",
      "Episode 79 Score 200.00\n",
      "Episode 80 Score 200.00\n",
      "Episode 81 Score 200.00\n",
      "Episode 82 Score 200.00\n",
      "Episode 83 Score 200.00\n",
      "Episode 84 Score 200.00\n",
      "Episode 85 Score 200.00\n",
      "Episode 86 Score 200.00\n",
      "Episode 87 Score 200.00\n",
      "Episode 88 Score 200.00\n",
      "Episode 89 Score 200.00\n",
      "Episode 90 Score 200.00\n",
      "Episode 91 Score 200.00\n",
      "Episode 92 Score 200.00\n",
      "Episode 93 Score 200.00\n",
      "Episode 94 Score 200.00\n",
      "Episode 95 Score 200.00\n",
      "Episode 96 Score 200.00\n",
      "Episode 97 Score 200.00\n",
      "Episode 98 Score 200.00\n",
      "Episode 99 Score 200.00\n",
      "Episode 100 Score 200.00\n",
      "Mean of 100 episodes = 200.0\n",
      "2019-02-08 10:48:11.737848\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "for i in range(100): # 10 episodes\n",
    "    state = env.reset()                                   # reset the environment\n",
    "    score = 0                                             # initialize the score\n",
    "    while True:\n",
    "        action = agent.act(state)                         # select an action\n",
    "        next_state, reward, done, info = env.step(action) # send the action to the environment\n",
    "        score += reward                                   # update the score\n",
    "        state = next_state                                # roll over the state to next time step\n",
    "        if done:                                          # exit loop if episode finished\n",
    "            break\n",
    "    scores.append(score)\n",
    "    print(\"Episode {:2d} Score {:5.2f}\".format(i+1,score))\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _That's all Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
